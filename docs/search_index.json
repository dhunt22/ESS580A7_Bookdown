[["index.html", "ESS 580A7 Introduction to Environmental Data Science Project Compilation Preface", " ESS 580A7 Introduction to Environmental Data Science Project Compilation Devin Hunt 2022-03-31 Preface Created for the ESS 580A7 course at Colorado State University. This is a compilation of modules applying various packages and skills in R. "],["discharge-data-example.html", "Chapter 1 Discharge Data Example 1.1 Site Description 1.2 Data Acquisition and Plotting tests 1.3 Data Download 1.4 Static Data Plotter 1.5 Interactive Data Plotter 1.6 Assignment 1.7 DyGraph example. 1.8 Poudre Paragraph", " Chapter 1 Discharge Data Example The Poudre River at Lincoln Bridge is: Downstream of only a little bit of urban storm water Near Odell Brewing CO Near an open space area and the Poudre River Trail Downstream of many agricultural diversions 1.1 Site Description 1.2 Data Acquisition and Plotting tests 1.3 Data Download q &lt;- readNWISdv(siteNumbers = &#39;06752260&#39;, parameterCd = &#39;00060&#39;, startDate = &#39;2017-01-01&#39;, endDate = &#39;2022-01-01&#39;) %&gt;% rename(q = &#39;X_00060_00003&#39;) 1.4 Static Data Plotter ggplot(q, aes(x = Date, y = q)) + geom_line() + ylab(&#39;Q (cfs)&#39;) + ggtitle(&#39;Discharge in the Poudre River, Fort Collins&#39;) 1.5 Interactive Data Plotter q_xts &lt;- xts(q$q, order.by = q$Date) dygraph(q_xts) %&gt;% dyAxis(&quot;y&quot;, label = &quot;Discharge (cfs)&quot;) 1.6 Assignment This assignment will be primarily about demonstrating some expertice in using RMarkdown, since we will be using Rmds as the primary form of homework and assignments. With that in mind, your assignment for this homework is to: Fork the example repository into your personal GitHub Create an RStudio project from your Personal clone of the Repo. Create a table of contents that is floating, but displays three levels of headers instead of two (by editing the content at the beginning of the document) Make a version of the dygraph with points and lines by using rstudios dygraph guide Writing a paragraph on the Poudre river with at least three hyperlinks, two bolded sections, and one italicized phrase. The content of this paragraph is not vital, but try to at least make it true and interesting, and, of course, dont plagiarize. Knit that document, and then git commit and push to your personal GitHub. Use the GitHub -&gt; Settings -&gt; Pages tab to create a website of your report. Bonus, make the timestamp in the header dynamic. As in it only adds todays date, not just a static date you enter. Bonus, create an index_talk.Rmd version of your document using the revealjs package. Add link to your original report-style document. 1.7 DyGraph example. q_xts &lt;- xts(q$q, order.by = q$Date) dygraph(q_xts, main = &quot;Discharge of the Cache La Poudre River At Fort Collins, CO&quot;) %&gt;% dyAxis(&quot;y&quot;, label = &quot;Discharge (cfs)&quot;) %&gt;% dyOptions(drawPoints = TRUE, pointSize = 2) 1.8 Poudre Paragraph The Cache La Poudre River, first settled against in the 1870s, provided water for mining, logging, and agriculture. Original settlements, such as Old Poudre City, began working on a railroad system along the central river. This project, however, failed, as there was a lack of support by the federal government. Highway 14 was created from the proposed railroad track. In the present and future, the Cache La Poudre River will continue to be a vital source of water to the South Platte River and agricultural operations around Fort Collins. Sources: Rivers.gov Description The Historical Marker Database A Better Future for the Poudre River library(tidyverse) library(tidyr) library(ggthemes) library(lubridate) # Now that we have learned how to munge (manipulate) data # and plot it, we will work on using these skills in new ways # # knitr::opts_knit$set(root.dir=&#39;..&#39;) "],["hayman-fire-recovery.html", "Chapter 2 Hayman Fire Recovery 2.1 Question 1) 2.2 Question 2 2.3 Q3 2.4 Question 4 2.5 Question 5)", " Chapter 2 Hayman Fire Recovery ####-----Reading in Data and Stacking it ----- #### #Reading in files files &lt;- list.files(&quot;./02-data&quot;, full.names=T) # If run from line, lowest directory: need ../ | If run from console, base directory at .Rproj paste(files) ## [1] &quot;./02-data/hayman_ndmi.csv&quot; &quot;./02-data/hayman_ndsi.csv&quot; &quot;./02-data/hayman_ndvi.csv&quot; #Read in individual data files ndmi &lt;- read_csv(files[1]) %&gt;% rename(burned=2,unburned=3) %&gt;% mutate(data=&#39;ndmi&#39;) ndsi &lt;- read_csv(files[2]) %&gt;% rename(burned=2,unburned=3) %&gt;% mutate(data=&#39;ndsi&#39;) ndvi &lt;- read_csv(files[3])%&gt;% rename(burned=2,unburned=3) %&gt;% mutate(data=&#39;ndvi&#39;) # Stack as a tidy dataset full_long &lt;- rbind(ndvi,ndmi,ndsi) %&gt;% gather(key=&#39;site&#39;,value=&#39;value&#39;,-DateTime,-data) %&gt;% filter(!is.na(value)) 2.1 Question 1) What is the correlation between NDVI and NDMI? - here I want you to convert the full_long dataset in to a wide dataset using the function spread and then make a plot that shows the correlations a function of if the site was burned or not (x axis should be ndmi) You should exclude winter months and focus on summer months wide_long &lt;- spread(full_long, key = &quot;data&quot;, value = &quot;value&quot;) %&gt;% mutate(month = month(DateTime)) %&gt;% filter(month %in% c(5:9)) ggplot(wide_long, aes(ndmi, ndvi, color = site)) + geom_point() ## Warning: Removed 19 rows containing missing values (geom_point). 2.2 Question 2 What is the correlation between average NDSI (normalized snow index) for January - April and average NDVI for June-August? In other words, does the previous years snow cover influence vegetation growth for the following summer? # Group by year (early NDVI / late NDSI). Compare the different years, not months ##Got help with this question on Monday, did not have the chance to complete it (02-07-2022) # Filter and prepare data to be aggregated by month, year. wide_early_ndsi &lt;- spread(full_long, key = &quot;data&quot;, value = &quot;value&quot;) %&gt;% mutate(month = month(DateTime)) %&gt;% mutate(year = year(DateTime)) %&gt;% filter(month %in% c(1:4)) %&gt;% select(site, ndsi, month, year) %&gt;% group_by(month, year) wide_late_ndvi &lt;- spread(full_long, key = &quot;data&quot;, value = &quot;value&quot;) %&gt;% mutate(month = month(DateTime)) %&gt;% mutate(year = year(DateTime)) %&gt;% filter(month %in% c(6:8)) %&gt;% select(site, ndvi, month, year) %&gt;% group_by(month, year) # Aggregate data values for ndsi/ndvi by year ndsi_yearly &lt;- aggregate(ndsi ~ year + site, wide_early_ndsi, FUN = mean) ndvi_yearly &lt;- aggregate(ndvi ~ year + site, wide_late_ndvi, FUN = mean) # Combine data for plotting total &lt;- full_join(ndsi_yearly, ndvi_yearly) ## Joining, by = c(&quot;year&quot;, &quot;site&quot;) # Plot of the data, color by type (ndvi, ndsi) ggplot() + geom_point(data = filter(total, site == &quot;burned&quot;), aes(year, ndsi, col = &quot;ndsi&quot;)) + geom_point(data = filter(total, site == &quot;unburned&quot;), aes(year, ndvi, col = &quot;ndvi&quot;)) + scale_x_continuous(n.breaks = 20) + scale_color_manual(values=c(&quot;cyan3&quot;, &quot;chartreuse4&quot;)) cor(total$ndsi, total$ndvi) ## [1] 0.1813135 Snowfall has a weak short-term impact on the later seasons vegetation index. We can see in the graph that years with lower snow indexes over a longer period cause a small change in the NDVI in the future years. 2.3 Q3 How is the snow effect from question 2 different between pre- and post-burn and burned and unburned? Significantly lower snow is detected in the initial post-burn area. From the graph, we can see that the index values drop significantly initially, the recover over the course of about 4 years. 2.4 Question 4 What month is the greenest month on average? # Month with highest NDVI on average, rename months ndvi_monthly &lt;- aggregate(ndvi ~ month, wide_late_ndvi, FUN = mean) %&gt;% mutate(month = month(month, label = TRUE, abbr = FALSE)) # Find max index max_ndvi &lt;- which.max(ndvi_monthly[,2]) # Print max index month name print(paste(ndvi_monthly[max_ndvi,1], &quot;is the greenest month on average.&quot;)) ## [1] &quot;August is the greenest month on average.&quot; 2.5 Question 5) What month is the snowiest on average? # Month with highest NDSI on average, rename months ndsi_monthly &lt;- aggregate(ndsi ~ month, wide_early_ndsi, FUN = mean) %&gt;% mutate(month = month(month, label = TRUE, abbr = FALSE)) # Find max index max_ndsi &lt;- which.max(ndsi_monthly[,2]) # Print max index month name print(paste(ndsi_monthly[max_ndsi,1], &quot;is the snowiest month on average.&quot;)) ## [1] &quot;January is the snowiest month on average.&quot; "],["snow-data-assignment-web-scraping-functions-and-iteration.html", "Chapter 3 Snow Data Assignment: Web Scraping, Functions, and Iteration 3.1 Reading an html 3.2 Data Download 3.3 Data read-in 3.4 Assignment:", " Chapter 3 Snow Data Assignment: Web Scraping, Functions, and Iteration R can read html using either rvest, xml, or xml2 packages. Here we are going to navigate to the Center for Snow and Avalance Studies Website and read a table in. This table contains links to data we want to programatically download for three sites. We dont know much about these sites, but they contain incredibly rich snow, temperature, and precip data. 3.1 Reading an html 3.1.1 Extract CSV links from webpage site_url &lt;- &#39;https://snowstudies.org/archived-data/&#39; #Read the web url webpage &lt;- read_html(site_url) #See if we can extract tables and get the data that way tables &lt;- webpage %&gt;% html_nodes(&#39;table&#39;) %&gt;% magrittr::extract2(3) %&gt;% html_table(fill = TRUE) #That didn&#39;t work, so let&#39;s try a different approach #Extract only weblinks and then the URLs! links &lt;- webpage %&gt;% html_nodes(&#39;a&#39;) %&gt;% .[grepl(&#39;24hr&#39;,.)] %&gt;% html_attr(&#39;href&#39;) 3.2 Data Download 3.2.1 Download data in a for loop #Grab only the name of the file by splitting out on forward slashes splits &lt;- str_split_fixed(links,&#39;/&#39;,8) #Keep only the 8th column dataset &lt;- splits[,8] #generate a file list for where the data goes file_names &lt;- paste0(&#39;03-data/&#39;, dataset) for(i in 1:3){ download.file(links[i],destfile=file_names[i]) } downloaded &lt;- file.exists(file_names) evaluate &lt;- !all(downloaded) 3.2.2 Download data in a map #Map version of the same for loop (downloading 3 files) if(evaluate == T){ map2(links[1:3],file_names[1:3],download.file) }else{print(&#39;data already downloaded&#39;)} ## [[1]] ## [1] 0 ## ## [[2]] ## [1] 0 ## ## [[3]] ## [1] 0 3.3 Data read-in 3.3.1 Read in just the snow data as a loop #Pattern matching to only keep certain files snow_files &lt;- file_names %&gt;% .[!grepl(&#39;SG_24&#39;,.)] %&gt;% .[!grepl(&#39;PTSP&#39;,.)] empty_data &lt;- list() snow_data &lt;- for(i in 1:length(snow_files)){ empty_data[[i]] &lt;- read_csv(snow_files[i]) %&gt;% select(Year,DOY,Sno_Height_M) } snow_data_full &lt;- do.call(&#39;rbind&#39;,empty_data) summary(snow_data_full) ## Year DOY Sno_Height_M ## Min. :2003 Min. : 1.0 Min. :-3.523 ## 1st Qu.:2008 1st Qu.: 92.0 1st Qu.: 0.350 ## Median :2012 Median :183.0 Median : 0.978 ## Mean :2012 Mean :183.1 Mean : 0.981 ## 3rd Qu.:2016 3rd Qu.:274.0 3rd Qu.: 1.520 ## Max. :2021 Max. :366.0 Max. : 2.905 ## NA&#39;s :4554 3.3.2 Read in the data as a map function our_snow_reader &lt;- function(file){ name = str_split_fixed(file,&#39;/&#39;,2)[,2] %&gt;% gsub(&#39;_24hr.csv&#39;,&#39;&#39;,.) df &lt;- read_csv(file) %&gt;% select(Year,DOY,Sno_Height_M) %&gt;% mutate(site = name) } snow_data_full &lt;- map_dfr(snow_files,our_snow_reader) summary(snow_data_full) ## Year DOY Sno_Height_M site ## Min. :2003 Min. : 1.0 Min. :-3.523 Length:12786 ## 1st Qu.:2008 1st Qu.: 92.0 1st Qu.: 0.350 Class :character ## Median :2012 Median :183.0 Median : 0.978 Mode :character ## Mean :2012 Mean :183.1 Mean : 0.981 ## 3rd Qu.:2016 3rd Qu.:274.0 3rd Qu.: 1.520 ## Max. :2021 Max. :366.0 Max. : 2.905 ## NA&#39;s :4554 3.3.3 Plot snow data snow_yearly &lt;- snow_data_full %&gt;% group_by(Year,site) %&gt;% summarize(mean_height = mean(Sno_Height_M,na.rm=T)) ggplot(snow_yearly,aes(x=Year,y=mean_height,color=site)) + geom_point() + ggthemes::theme_few() + ggthemes::scale_color_few() 3.4 Assignment: Extract the meteorological data URLs. Here we want you to use the rvest package to get the URLs for the SASP forcing and SBSP_forcing meteorological datasets. links_forcing &lt;- webpage %&gt;% html_nodes(&#39;a&#39;) %&gt;% .[grepl(&#39;forcing&#39;,.)] %&gt;% html_attr(&#39;href&#39;) Download the meteorological data. Use the download_file and str_split_fixed commands to download the data and save it in your data folder. You can use a for loop or a map function. #Grab only the name of the file by splitting out on forward slashes splits2 &lt;- str_split_fixed(links_forcing,&#39;/&#39;,8) #Keep only the 8th column dataset2 &lt;- splits2[,8] #generate a file list for where the data goes file_names2 &lt;- paste0(&#39;03-data/&#39;, dataset2) for(i in 1:2){ download.file(links_forcing[i], destfile=file_names2[i]) } Write a custom function to read in the data and append a site column to the data. # this code grabs the variable names from the metadata pdf file library(pdftools) headers &lt;- pdf_text(&#39;https://snowstudies.org/wp-content/uploads/2022/02/Serially-Complete-Metadata-text08.pdf&#39;) %&gt;% readr::read_lines(.) %&gt;% trimws(.) %&gt;% str_split_fixed(.,&#39;\\\\.&#39;,2) %&gt;% .[,2] %&gt;% .[1:26] %&gt;% str_trim(side = &quot;left&quot;) forcing_snow_reader &lt;- function(dfile){ name = str_split_fixed(dfile,&#39;/&#39;,2)[,2] %&gt;% gsub(&#39;_Forcing_Data.txt&#39;,&#39;&#39;,.) %&gt;% gsub(&#39;SBB_&#39;,&#39;&#39;,.) df &lt;- read_table(dfile, col_names = headers, na = &quot;NA&quot;) # When using mutate # df &lt;- mutate(site = name) # Continued to recieve the error: # Error in UseMethod(&quot;mutate&quot;) : no applicable method for &#39;mutate&#39; applied to an object of class &quot;character&quot; # Solution: df %&gt;% add_column(site = name) } Use the map function to read in both meteorological files. Display a summary of your tibble. # Attempted renaming strategies: # names(df) &lt;- headers # # select(Year,DOY,Sno_Height_M) # rename_with(headers) %&gt;% forcing_data_full &lt;- map_dfr(file_names2, forcing_snow_reader) ## ## -- Column specification -------------------------------------------------------------------------------------- ## cols( ## .default = col_double(), ## `qc code precip` = col_character(), ## `qc code sw down` = col_character(), ## `qc code lw down` = col_character(), ## `qc code air temp` = col_character(), ## `qc code wind speed` = col_character(), ## `qc code relhum` = col_character() ## ) ## i Use `spec()` for the full column specifications. ## ## ## -- Column specification -------------------------------------------------------------------------------------- ## cols( ## .default = col_double(), ## `qc code precip` = col_character(), ## `qc code sw down` = col_character(), ## `qc code lw down` = col_character(), ## `qc code air temp` = col_character(), ## `qc code wind speed` = col_character(), ## `qc code relhum` = col_character() ## ) ## i Use `spec()` for the full column specifications. summary(forcing_data_full) ## year month day hour minute second precip [kg m-2 s-1] ## Min. :2003 Min. : 1.000 Min. : 1.00 Min. : 0.00 Min. :0 Min. :0 Min. :0.000000 ## 1st Qu.:2005 1st Qu.: 3.000 1st Qu.: 8.00 1st Qu.: 5.75 1st Qu.:0 1st Qu.:0 1st Qu.:0.000000 ## Median :2007 Median : 6.000 Median :16.00 Median :11.50 Median :0 Median :0 Median :0.000000 ## Mean :2007 Mean : 6.472 Mean :15.76 Mean :11.50 Mean :0 Mean :0 Mean :0.000038 ## 3rd Qu.:2009 3rd Qu.: 9.000 3rd Qu.:23.00 3rd Qu.:17.25 3rd Qu.:0 3rd Qu.:0 3rd Qu.:0.000000 ## Max. :2011 Max. :12.000 Max. :31.00 Max. :23.00 Max. :0 Max. :0 Max. :0.006111 ## NA&#39;s :8 NA&#39;s :8 NA&#39;s :8 NA&#39;s :8 NA&#39;s :8 NA&#39;s :8 NA&#39;s :8 ## sw down [W m-2] lw down [W m-2] air temp [K] windspeed [m s-1] relative humidity [%] ## Min. :-9999.000 Min. :-9999.0 Min. :242.1 Min. :-9999.000 Min. : 0.011 ## 1st Qu.: -3.510 1st Qu.: 173.4 1st Qu.:265.8 1st Qu.: 0.852 1st Qu.: 37.580 ## Median : -0.344 Median : 231.4 Median :272.6 Median : 1.548 Median : 59.910 ## Mean :-1351.008 Mean :-1325.7 Mean :272.6 Mean : -790.054 Mean : 58.891 ## 3rd Qu.: 294.900 3rd Qu.: 272.2 3rd Qu.:279.7 3rd Qu.: 3.087 3rd Qu.: 81.600 ## Max. : 1341.000 Max. : 365.8 Max. :295.8 Max. : 317.300 Max. :324.800 ## NA&#39;s :8 NA&#39;s :8 NA&#39;s :8 NA&#39;s :8 NA&#39;s :8 ## pressure [Pa] specific humidity [g g-1] calculated dewpoint temperature [K] ## Min. :63931 Min. :0.000000 Min. : 0.0 ## 1st Qu.:63931 1st Qu.:0.001744 1st Qu.: 0.0 ## Median :65397 Median :0.002838 Median : 0.0 ## Mean :65397 Mean :0.003372 Mean : 74.9 ## 3rd Qu.:66863 3rd Qu.:0.004508 3rd Qu.: 0.0 ## Max. :66863 Max. :0.014780 Max. :2002.0 ## NA&#39;s :8 NA&#39;s :8 NA&#39;s :8 ## precip, WMO-corrected [kg m-2 s-1] air temp, corrected with Kent et al. (1993) [K] ## Min. : 0.0 Min. : 0 ## 1st Qu.: 0.0 1st Qu.: 0 ## Median : 0.0 Median : 0 ## Mean : 424.7 Mean : 438 ## 3rd Qu.: 0.0 3rd Qu.: 0 ## Max. :3002.0 Max. :5002 ## NA&#39;s :8 NA&#39;s :8 ## air temp, corrected with Anderson and Baumgartner (1998)[K] ## Min. : 0.0 ## 1st Qu.: 0.0 ## Median : 0.0 ## Mean : 173.9 ## 3rd Qu.: 0.0 ## Max. :5002.0 ## NA&#39;s :8 ## air temp, corrected with Nakamura and Mahrt (2005) [K] air temp, corrected with Huwald et al. (2009) [K] ## Min. : 0.0 Min. : 0.00 ## 1st Qu.: 0.0 1st Qu.: 0.00 ## Median : 0.0 Median : 0.00 ## Mean : 605.9 Mean : 56.49 ## 3rd Qu.: 0.0 3rd Qu.: 0.00 ## Max. :6002.0 Max. :6009.00 ## NA&#39;s :8 NA&#39;s :5222 ## qc code precip qc code sw down qc code lw down qc code air temp qc code wind speed ## Length:138344 Length:138344 Length:138344 Length:138344 Length:138344 ## Class :character Class :character Class :character Class :character Class :character ## Mode :character Mode :character Mode :character Mode :character Mode :character ## ## ## ## ## qc code relhum site ## Length:138344 Length:138344 ## Class :character Class :character ## Mode :character Mode :character ## ## ## ## Make a line plot of mean temp by year by site (using the air temp [K] variable). Is there anything suspicious in the plot? Adjust your filtering if needed. mean_temp_year &lt;- forcing_data_full %&gt;% group_by(year, site) %&gt;% summarize(mean_temp = mean(`air temp [K]`, na.rm = TRUE)) ggplot(mean_temp_year, aes(year, mean_temp, color = site)) + geom_line() + labs(title = &quot;Mean Temperature [K] By Year&quot;, x = &quot;Year&quot;, y = &quot;Mean Temp [Kelvin]&quot;) The year 2003 produced abnormally low mean temperatures for both sites. Write a function that makes line plots of monthly average temperature at each site for a given year. Use a for loop to make these plots for 2005 to 2010. Are monthly average temperatures at the Senator Beck Study Plot ever warmer than the Snow Angel Study Plot? Hint: https://ggplot2.tidyverse.org/reference/print.ggplot.html monthly_temp &lt;- function(data, year_value){ dataf &lt;- data %&gt;% filter(year == year_value) month_values &lt;- dataf %&gt;% group_by(month, site) %&gt;% summarize(mean_temp = mean(`air temp [K]`, na.rm = TRUE)) plot_title &lt;- paste(&quot;Mean Temperature [K] By Month in&quot;, year_value) p1 &lt;- ggplot(month_values, aes(month, mean_temp, color = site)) + geom_line() + labs(title = plot_title, x = &quot;Month&quot;, y = &quot;Mean Temp [Kelvin]&quot;) print(p1) } year_input &lt;- c(2005, 2006, 2007, 2008, 2009, 2010) for (i in 1:length(year_input)){ monthly_temp_plots &lt;- monthly_temp(forcing_data_full, year_input[i]) } From the 6 graphs created, we cannot find a monthly temperature observation where SBSP exceeds the SASP site. Bonus: Make a plot of average daily precipitation by day of year (averaged across all available years). Color each site. # Create a date from given days forcing_dates &lt;- forcing_data_full %&gt;% mutate(Date = make_date(year, month, day)) %&gt;% filter(!is.na(Date)) %&gt;% mutate(day_val = yday(Date)) daily_values &lt;- forcing_dates %&gt;% group_by(day_val, site) %&gt;% summarize(mean_precip = mean(`precip [kg m-2 s-1]`, na.rm = TRUE)) p2 &lt;- ggplot(daily_values, aes(day_val, mean_precip, color = site)) + geom_line() + labs(title = &quot;Average Daily Precipitation by Day of Year&quot;, x = &quot;Day&quot;, y = &quot;Precip [kg m-2 s-1]&quot;) print(p2) Bonus #2: Use a function and for loop to create yearly plots of precipitation by day of year. Color each site. daily_precip &lt;- function(precip_data, year_value){ precip_dataf &lt;- precip_data %&gt;% filter(year == year_value) day_values &lt;- precip_dataf %&gt;% group_by(day_val, site) %&gt;% summarize(mean_year_precip = mean(`precip [kg m-2 s-1]`, na.rm = TRUE)) plot_title &lt;- paste(&quot;Mean Precipitation [kg m-2 s-1] By Date in&quot;, year_value) p3 &lt;- ggplot(day_values, aes(day_val, mean_year_precip, color = site)) + geom_line() + labs(title = plot_title, x = &quot;Date&quot;, y = &quot;Mean Temp [Kelvin]&quot;) print(p3) } year_input &lt;- c(2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011) for (i in 1:length(year_input)){ daily_precip_plots &lt;- daily_precip(forcing_dates, year_input[i]) } "],["lagos-spatial-analysis.html", "Chapter 4 LAGOS Spatial Analysis 4.1 Loading in data 4.2 In-Class work 4.3 1) Show a map outline of Iowa and Illinois (similar to Minnesota map upstream) 4.4 2) Subset LAGOS data to these sites, how many sites are in Illinois and Iowa combined? How does this compare to Minnesota? 4.5 3) What is the distribution of lake size in Iowa vs. Minnesota? 4.6 4) Make an interactive plot of lakes in Iowa and Illinois and color them by lake area in hectares 4.7 5) What other data sources might we use to understand how reservoirs and natural lakes vary in size in these three states?", " Chapter 4 LAGOS Spatial Analysis 4.1 Loading in data 4.1.1 First download and then specifically grab the locus (or site lat longs) # #Lagos download script # LAGOSNE::lagosne_get(dest_folder = LAGOSNE:::lagos_path()) #Load in lagos lagos &lt;- lagosne_load() #Grab the lake centroid info lake_centers &lt;- lagos$locus # load(&#39;lake_centers.Rdata&#39;) 4.1.2 Convert to spatial data #Look at the column names #names(lake_centers) #Look at the structure #str(lake_centers) #View the full dataset #View(lake_centers %&gt;% slice(1:100)) spatial_lakes &lt;- st_as_sf(lake_centers,coords=c(&#39;nhd_long&#39;,&#39;nhd_lat&#39;), crs=4326) %&gt;% st_transform(2163) #Subset for plotting subset_spatial &lt;- spatial_lakes %&gt;% slice(1:100) subset_baser &lt;- spatial_lakes[1:100,] #Dynamic mapviewer mapview(subset_spatial) 4.1.3 Subset to only Minnesota states &lt;- us_states() #Plot all the states to check if they loaded #mapview(states) minnesota &lt;- states %&gt;% filter(name == &#39;Minnesota&#39;) %&gt;% st_transform(2163) #Subset lakes based on spatial position, mutate state name minnesota_lakes &lt;- spatial_lakes[minnesota,] %&gt;% mutate(name = &quot;Minnesota&quot;) #Plotting the first 1000 lakes minnesota_lakes %&gt;% arrange(-lake_area_ha) %&gt;% slice(1:1000) %&gt;% mapview(.,zcol = &#39;lake_area_ha&#39;) 4.2 In-Class work 4.3 1) Show a map outline of Iowa and Illinois (similar to Minnesota map upstream) # Grab both Iowa and Illinois state codes iowa_illinois &lt;- states %&gt;% filter(name %in% c(&quot;Iowa&quot;,&quot;Illinois&quot;)) %&gt;% st_transform(2163) # Isolate lakes from Iowa and Illinois iowa_illinois_lakes &lt;- spatial_lakes[iowa_illinois,] # Display the Outline mapview(iowa_illinois, col.regions = &quot;chartreuse4&quot;) 4.4 2) Subset LAGOS data to these sites, how many sites are in Illinois and Iowa combined? How does this compare to Minnesota? # Give an output statement on the sites in Iowa + Illinois vs. Minnesota print(paste(&quot;There are&quot;, nrow(iowa_illinois_lakes), &quot;lakes in Iowa and Illinois, and&quot;, nrow(minnesota_lakes), &quot;lakes in Minnesota.&quot;)) ## [1] &quot;There are 16466 lakes in Iowa and Illinois, and 29038 lakes in Minnesota.&quot; 4.5 3) What is the distribution of lake size in Iowa vs. Minnesota? Here I want to see a histogram plot with lake size on x-axis and frequency on y axis (check out geom_histogram) # Isolate Iowa lakes and mutate state name iowa &lt;- iowa_illinois %&gt;% filter(name == &quot;Iowa&quot;) iowa_lakes &lt;- spatial_lakes[iowa,] %&gt;% mutate(name = &quot;Iowa&quot;) iowa_minnesota_lakes &lt;- rbind(iowa_lakes, minnesota_lakes) p1 &lt;- ggplot(iowa_lakes, aes(lake_area_ha)) + geom_histogram(position = &quot;dodge&quot;, color = &quot;darkblue&quot;, fill = &quot;dodgerblue4&quot;, binwidth = 0.04) + theme(axis.title.x = element_blank()) + scale_x_log10() + labs(title = &quot;Distibution of Iowa lakes&quot;, y = &quot;Count&quot;) p2 &lt;- ggplot(minnesota_lakes, aes(lake_area_ha), ) + geom_histogram(position = &quot;dodge&quot;, color = &quot;darkblue&quot;, fill = &quot;dodgerblue4&quot;, binwidth = 0.06) + theme() + scale_x_log10() + labs(title = &quot;Distibution of Minnesota lakes&quot;, y = &quot;Count&quot;, x = &quot;Lake Area (Hectares)&quot;) grid::grid.draw(rbind(ggplotGrob(p1), ggplotGrob(p2))) ## ^First solution before Matt told me to facet wrap. # Facet wrap graph by state name. p3 &lt;- ggplot(iowa_minnesota_lakes, aes(x = lake_area_ha)) + geom_histogram(position = &quot;dodge&quot;, color = &quot;darkblue&quot;, fill = &quot;dodgerblue4&quot;, binwidth = 0.04) + scale_x_log10() + labs(title = &quot;Distibution of Minnesota and Iowa lakes&quot;, y = &quot;Count&quot;, x = &quot;Lake Area (Hectares)&quot;) + facet_wrap(~name, ncol = 1) p3 4.6 4) Make an interactive plot of lakes in Iowa and Illinois and color them by lake area in hectares # Interactive map of Iowa and Illinois iowa_illinois_lakes %&gt;% arrange(-lake_area_ha) %&gt;% mapview(., zcol = &#39;lake_area_ha&#39;, alpha = 0.05, cex = 2.0) 4.7 5) What other data sources might we use to understand how reservoirs and natural lakes vary in size in these three states? We may use the USGS water body databases, and stream flow data to see where water is travelling and being collected. Each state has data available. Additionally, we can use the NHD Watershed tool to see basin collections. A larger basin will likely host larger bodies of water, and the opposite for small basins. "],["lake-water-quality-analysis.html", "Chapter 5 Lake Water Quality Analysis 5.1 Loading in data 5.2 Class work 5.3 1) What is the correlation between Secchi Disk Depth and Chlorophyll a for sites with at least 200 observations? 5.4 Why might this be the case? 5.5 2) What states have the most data? 5.6 3) Is there a spatial pattern in Secchi disk depth for lakes with at least 200", " Chapter 5 Lake Water Quality Analysis 5.1 Loading in data 5.1.1 First download and then specifically grab the locus (or site lat longs) #Lagos download script #lagosne_get(dest_folder = LAGOSNE:::lagos_path(),overwrite=T) #Load in lagos lagos &lt;- lagosne_load() #Grab the lake centroid info lake_centers &lt;- lagos$locus # Make an sf object spatial_lakes &lt;- st_as_sf(lake_centers,coords=c(&#39;nhd_long&#39;,&#39;nhd_lat&#39;), crs=4326) #Grab the water quality data nutr &lt;- lagos$epi_nutr #Look at column names #names(nutr) 5.1.2 Subset columns nutr to only keep key info that we want clarity_only &lt;- nutr %&gt;% select(lagoslakeid,sampledate,chla,doc,secchi) %&gt;% mutate(sampledate = as.character(sampledate) %&gt;% ymd(.)) 5.1.3 Keep sites with at least 200 observations #Look at the number of rows of dataset #nrow(clarity_only) chla_secchi &lt;- clarity_only %&gt;% filter(!is.na(chla), !is.na(secchi)) # How many observations did we lose? # nrow(clarity_only) - nrow(chla_secchi) # Keep only the lakes with at least 200 observations of secchi and chla chla_secchi_200 &lt;- chla_secchi %&gt;% group_by(lagoslakeid) %&gt;% mutate(count = n()) %&gt;% filter(count &gt; 200) 5.1.4 Join water quality data to spatial data spatial_200 &lt;- inner_join(spatial_lakes,chla_secchi_200 %&gt;% distinct(lagoslakeid,.keep_all=T), by=&#39;lagoslakeid&#39;) 5.1.5 Mean Chl_a map ### Take the mean chl_a and secchi by lake mean_values_200 &lt;- chla_secchi_200 %&gt;% # Take summary by lake id group_by(lagoslakeid) %&gt;% # take mean chl_a per lake id summarize(mean_chl = mean(chla,na.rm=T), mean_secchi=mean(secchi,na.rm=T)) %&gt;% #Get rid of NAs filter(!is.na(mean_chl), !is.na(mean_secchi)) %&gt;% # Take the log base 10 of the mean_chl mutate(log10_mean_chl = log10(mean_chl)) #Join datasets mean_spatial &lt;- inner_join(spatial_lakes,mean_values_200, by=&#39;lagoslakeid&#39;) #Make a map mapview(mean_spatial,zcol=&#39;log10_mean_chl&#39;) 5.2 Class work 5.3 1) What is the correlation between Secchi Disk Depth and Chlorophyll a for sites with at least 200 observations? Here, I just want a plot of chla vs secchi for all sites #Your code here ggplot(chla_secchi_200, aes(x = chla, y = secchi)) + geom_point() + scale_x_log10() + labs(title = &quot;Correlation between Chlorophyll a and Depth to Secchi plate&quot;) 5.4 Why might this be the case? The correlation is slight negative correlation between CHLa and Secchi for the sites. At high levels of Chlorophyll a in the waterm the Secchi value increases. Therefore, as CHLa (chlorophyll plant presence) increases, the depth at which a Secchi Disk disappears out of view at a more shallow depth. This makes sense, as Chlorophyll will absorb more sunlight and wavelengths of the light spectrum. 5.5 2) What states have the most data? 5.5.1 2a) First you will need to make a lagos spatial dataset that has the total number of counts per site. ## Your code here # site_counts &lt;- clarity_only %&gt;% group_by(lagoslakeid) %&gt;% summarize(count = n()) # Had to summarize, using mutate caused 3 repeats of each site type # Inner join to match sites site_sf_counts &lt;- inner_join(spatial_lakes, site_counts, by=&#39;lagoslakeid&#39;) 5.5.2 2b) Second, you will need to join this point dataset to the us_boundaries data. ## Your code here states &lt;- us_states() # Spatial Join, keeping only United States sites sites_us &lt;- st_join(states, site_sf_counts) %&gt;% na.omit() 5.5.3 2c) Then you will want to group by state and sum all the observations in that state and arrange that data from most to least total observations per state. ## Your code here. # Grouping by state using group state_totals &lt;- sites_us %&gt;% group_by(state_name) %&gt;% summarize(state_count = sum(count)) # Using order in descending order &#39;-&#39; state_sorted &lt;- state_totals[order(-state_totals$state_count),] 5.6 3) Is there a spatial pattern in Secchi disk depth for lakes with at least 200 observations? ## Your code here # Mapview of sites in which the Secchi observation count is &gt; 200 mapview(spatial_200, zcol = &quot;secchi&quot;, alpha = 0.8) There is a spatial pattern between the lakes with over 200 Secchi observations. We can see that more Northern lakes have, on average, a greater Secchi depth value. Furthermore, the Northeast contains lakes with the highest overall Secchi values. "],["weather-and-corn-yield-regressions.html", "Chapter 6 Weather and Corn Yield Regressions 6.1 Temperature trends 6.2 Assignment", " Chapter 6 Weather and Corn Yield Regressions 6.0.1 Load the PRISM daily maximum temperatures # daily max temperature # dimensions: counties x days x years prism &lt;- readMat(&quot;05-data/prismiowa.mat&quot;) # look at county #1 t_1981_c1 &lt;- prism$tmaxdaily.iowa[1,,1] t_1981_c1[366] ## [1] NaN plot(1:366, t_1981_c1, type = &quot;l&quot;) ggplot() + geom_line(mapping = aes(x=1:366, y = t_1981_c1)) + theme_bw() + xlab(&quot;day of year&quot;) + ylab(&quot;daily maximum temperature (°C)&quot;) + ggtitle(&quot;Daily Maximum Temperature, Iowa County #1&quot;) ## Warning: Removed 1 row(s) containing missing values (geom_path). # assign dimension names to tmax matrix dimnames(prism$tmaxdaily.iowa) &lt;- list(prism$COUNTYFP, 1:366, prism$years) # converted 3d matrix into a data frame tmaxdf &lt;- as.data.frame.table(prism$tmaxdaily.iowa) # relabel the columns colnames(tmaxdf) &lt;- c(&quot;countyfp&quot;,&quot;doy&quot;,&quot;year&quot;,&quot;tmax&quot;) tmaxdf &lt;- tibble(tmaxdf) 6.1 Temperature trends 6.1.1 Summer temperature trends: Winneshiek County tmaxdf$doy &lt;- as.numeric(tmaxdf$doy) tmaxdf$year &lt;- as.numeric(as.character(tmaxdf$year)) winnesummer &lt;- tmaxdf %&gt;% filter(countyfp==191 &amp; doy &gt;= 152 &amp; doy &lt;= 243) %&gt;% group_by(year) %&gt;% summarize(meantmax = mean(tmax)) ggplot(winnesummer, mapping = aes(x = year, y = meantmax)) + geom_point() + theme_bw() + labs(x = &quot;year&quot;, y = &quot;Tmax (°C)&quot;) + geom_smooth(method = lm) ## `geom_smooth()` using formula &#39;y ~ x&#39; lm_summertmax &lt;- lm(meantmax ~ year, winnesummer) summary(lm_summertmax) ## ## Call: ## lm(formula = meantmax ~ year, data = winnesummer) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.5189 -0.7867 -0.0341 0.6859 3.7415 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 41.57670 36.44848 1.141 0.262 ## year -0.00747 0.01823 -0.410 0.684 ## ## Residual standard error: 1.232 on 36 degrees of freedom ## Multiple R-squared: 0.004644, Adjusted R-squared: -0.02301 ## F-statistic: 0.168 on 1 and 36 DF, p-value: 0.6844 6.1.2 Winter Temperatures - Winneshiek County winnewinter &lt;- tmaxdf %&gt;% filter(countyfp==191 &amp; (doy &lt;= 59 | doy &gt;= 335) &amp; !is.na(tmax)) %&gt;% group_by(year) %&gt;% summarize(meantmax = mean(tmax)) ggplot(winnewinter, mapping = aes(x = year, y = meantmax)) + geom_point() + theme_bw() + labs(x = &quot;year&quot;, y = &quot;Tmax (°C)&quot;) + geom_smooth(method = lm) lm_wintertmax &lt;- lm(meantmax ~ year, winnewinter) summary(lm_wintertmax) ## ## Call: ## lm(formula = meantmax ~ year, data = winnewinter) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.5978 -1.4917 -0.3053 1.3778 4.5709 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -29.87825 60.48100 -0.494 0.624 ## year 0.01368 0.03025 0.452 0.654 ## ## Residual standard error: 2.045 on 36 degrees of freedom ## Multiple R-squared: 0.005652, Adjusted R-squared: -0.02197 ## F-statistic: 0.2046 on 1 and 36 DF, p-value: 0.6537 6.1.3 Multiple regression  Quadratic time trend winnewinter$yearsq &lt;- winnewinter$year^2 lm_wintertmaxquad &lt;- lm(meantmax ~ year + yearsq, winnewinter) summary(lm_wintertmaxquad) ## ## Call: ## lm(formula = meantmax ~ year + yearsq, data = winnewinter) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.3539 -1.2985 -0.2813 1.4055 4.2620 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.086e+04 1.238e+04 -0.877 0.386 ## year 1.085e+01 1.239e+01 0.876 0.387 ## yearsq -2.710e-03 3.097e-03 -0.875 0.388 ## ## Residual standard error: 2.051 on 35 degrees of freedom ## Multiple R-squared: 0.02694, Adjusted R-squared: -0.02867 ## F-statistic: 0.4845 on 2 and 35 DF, p-value: 0.6201 winnewinter$fitted &lt;- lm_wintertmaxquad$fitted.values ggplot(winnewinter) + geom_point(mapping = aes(x = year, y = meantmax)) + geom_line(mapping = aes(x = year, y = fitted)) + theme_bw() + labs(x = &quot;year&quot;, y = &quot;tmax&quot;) 6.1.4 Download NASS corn yield data # set our API key with NASS from source api file source(&quot;C:/Users/devin/Desktop/CSU/S22/ESS 580A7/Bookdown/api-key.R&quot;) nassqs_auth(key = nass_api_key) # parameters to query on params &lt;- list(commodity_desc = &quot;CORN&quot;, util_practice_desc = &quot;GRAIN&quot;, prodn_practice_desc = &quot;ALL PRODUCTION PRACTICES&quot;, year__GE = 1981, state_alpha = &quot;IA&quot;) # download cornyieldsall &lt;- nassqs_yields(params) ## | | | 0% | |= | 1% | |== | 2% | |=== | 3% | |==== | 4% | |===== | 5% | |====== | 6% | |======= | 7% | |======== | 8% | |========= | 9% | |========== | 10% | |=========== | 11% | |============ | 12% | |============= | 13% | |============== | 14% | |=============== | 15% | |================ | 16% | |================= | 17% | |================== | 18% | |=================== | 19% | |==================== | 20% | |===================== | 21% | |====================== | 22% | |======================= | 23% | |======================== | 24% | |========================= | 25% | |========================== | 26% | |=========================== | 27% | |============================ | 28% | |============================= | 29% | |============================== | 30% | |=============================== | 31% | |================================ | 32% | |================================= | 33% | |================================== | 34% | |=================================== | 35% | |==================================== | 36% | |===================================== | 37% | |====================================== | 38% | |======================================= | 39% | |======================================== | 40% | |========================================= | 41% | |========================================== | 42% | |=========================================== | 43% | |============================================ | 44% | |============================================= | 45% | |============================================== | 46% | |=============================================== | 47% | |================================================ | 48% | |================================================= | 49% | |================================================== | 50% | |=================================================== | 51% | |==================================================== | 52% | |===================================================== | 53% | |====================================================== | 54% | |======================================================= | 55% | |======================================================== | 56% | |========================================================= | 57% | |========================================================== | 58% | |=========================================================== | 59% | |============================================================ | 60% | |============================================================= | 61% | |============================================================== | 62% | |=============================================================== | 63% | |================================================================ | 64% | |================================================================= | 65% | |================================================================== | 66% | |=================================================================== | 67% | |==================================================================== | 68% | |===================================================================== | 69% | |====================================================================== | 70% | |======================================================================= | 71% | |======================================================================== | 72% | |========================================================================= | 73% | |========================================================================== | 74% | |=========================================================================== | 75% | |============================================================================ | 76% | |============================================================================= | 77% | |============================================================================== | 78% | |=============================================================================== | 79% | |================================================================================ | 80% | |================================================================================= | 81% | |================================================================================== | 82% | |=================================================================================== | 83% | |==================================================================================== | 84% | |===================================================================================== | 85% | |====================================================================================== | 86% | |======================================================================================= | 87% | |======================================================================================== | 88% | |========================================================================================= | 89% | |========================================================================================== | 90% | |=========================================================================================== | 91% | |============================================================================================ | 92% | |============================================================================================= | 93% | |============================================================================================== | 94% | |=============================================================================================== | 95% | |================================================================================================ | 96% | |================================================================================================= | 97% | |================================================================================================== | 98% | |=================================================================================================== | 99% | |====================================================================================================| 100% cornyieldsall$county_ansi &lt;- as.numeric(cornyieldsall$county_ansi) cornyieldsall$yield &lt;- as.numeric(cornyieldsall$Value) # clean and filter this dataset cornyields &lt;- select(cornyieldsall, county_ansi, county_name, yield, year) %&gt;% filter(!is.na(county_ansi) &amp; !is.na(yield)) cornyields &lt;- tibble(cornyields) 6.2 Assignment 6.2.1 Question 1a: Extract Winneshiek County corn yields, fit a linear time trend, make a plot. Is there a significant time trend? winne_yield &lt;- cornyields %&gt;% filter(county_ansi == 191) winne_lm &lt;- lm(yield ~ year, winne_yield) summary(winne_lm) ## ## Call: ## lm(formula = yield ~ year, data = winne_yield) ## ## Residuals: ## Min 1Q Median 3Q Max ## -51.163 -1.841 2.363 9.437 24.376 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -4763.290 448.286 -10.63 4.46e-13 *** ## year 2.457 0.224 10.96 1.77e-13 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 16.97 on 39 degrees of freedom ## Multiple R-squared: 0.7551, Adjusted R-squared: 0.7488 ## F-statistic: 120.2 on 1 and 39 DF, p-value: 1.767e-13 ggplot(winne_yield, aes(year, yield)) + geom_point(color = &quot;darkgoldenrod1&quot;) + geom_smooth(method = lm, color = &quot;green4&quot;) + labs(title = &quot;Average Annual Corn Yield in Winneshank, Iowa [Linear Fit]&quot;, x = &quot;Year&quot;, y = &quot;Yield&quot;) 6.2.2 Question 1b: Fit a quadratic time trend (i.e., year + year^2) and make a plot. Is there evidence for slowing yield growth? winne_yield$yearsq &lt;- winne_yield$year^2 winne_quad &lt;- lm(yield ~ year + yearsq, winne_yield) summary(winne_quad) ## ## Call: ## lm(formula = yield ~ year + yearsq, data = winne_yield) ## ## Residuals: ## Min 1Q Median 3Q Max ## -51.384 -3.115 1.388 9.743 25.324 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.583e+04 8.580e+04 0.301 0.765 ## year -2.812e+01 8.576e+01 -0.328 0.745 ## yearsq 7.641e-03 2.143e-02 0.357 0.723 ## ## Residual standard error: 17.17 on 38 degrees of freedom ## Multiple R-squared: 0.7559, Adjusted R-squared: 0.7431 ## F-statistic: 58.84 on 2 and 38 DF, p-value: 2.311e-12 winne_yield$quadraticfit &lt;- winne_quad$fitted.values ggplot(winne_yield, aes(year, yield)) + geom_point(color = &quot;darkgoldenrod1&quot;) + geom_line(aes(year, quadraticfit), color = &quot;green4&quot;, size = 0.8) + labs(title = &quot;Average Annual Corn Yield in Winneshank, Iowa [Quadratic Fit]&quot;, x = &quot;Year&quot;, y = &quot;Yield&quot;) 6.2.3 Question 2  Time Series: Lets analyze the relationship between temperature and yields for the Winneshiek County time series. Use data on yield and summer avg Tmax. Is adding year or Tmax^2 to your model helpful? Make a plot and interpret the results. winne_yield_temp &lt;- inner_join(winne_yield, winnesummer, by = &quot;year&quot;) %&gt;% select(!quadraticfit) winne_YT_lm &lt;- lm(yield ~ meantmax, winne_yield_temp) summary(winne_YT_lm) ## ## Call: ## lm(formula = yield ~ meantmax, data = winne_yield_temp) ## ## Residuals: ## Min 1Q Median 3Q Max ## -71.96 -19.85 -3.19 24.64 61.72 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 275.876 118.335 2.331 0.0255 * ## meantmax -4.763 4.438 -1.073 0.2902 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 32.88 on 36 degrees of freedom ## Multiple R-squared: 0.03101, Adjusted R-squared: 0.004098 ## F-statistic: 1.152 on 1 and 36 DF, p-value: 0.2902 # Linear model is less effective at showing the proper trend between yield and mean max temperature # Creating a quadratic model for yield. winne_yield_temp$yieldsq &lt;- winne_yield_temp$yield^2 winne_yield_temp$meantsq &lt;- winne_yield_temp$meantmax^2 winne_YT_quad &lt;- lm(yield ~ meantmax + meantsq, winne_yield_temp) summary(winne_YT_lm) ## ## Call: ## lm(formula = yield ~ meantmax, data = winne_yield_temp) ## ## Residuals: ## Min 1Q Median 3Q Max ## -71.96 -19.85 -3.19 24.64 61.72 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 275.876 118.335 2.331 0.0255 * ## meantmax -4.763 4.438 -1.073 0.2902 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 32.88 on 36 degrees of freedom ## Multiple R-squared: 0.03101, Adjusted R-squared: 0.004098 ## F-statistic: 1.152 on 1 and 36 DF, p-value: 0.2902 winne_yield_temp$tmaxfit &lt;- winne_YT_quad$fitted.values ggplot(winne_yield_temp, aes(meantmax, yield)) + geom_point(color = &quot;darkgoldenrod1&quot;) + geom_line(aes(meantmax, tmaxfit), color = &quot;green4&quot;) + labs(title = &quot;Comparison of Corn Yield to Mean Annual Max Temperature, \\nWinneshank, Iowa [Quadratic Fit]&quot;, x = &quot;Mean Annual Max Temperature (C)&quot;, y = &quot;Yield&quot;) 6.2.4 Question 3  Cross-Section: Analyze the relationship between temperature and yield across all counties in 2018. Is there a relationship? Interpret the results. # Filter for 2018 data, convert county to factor yield_2018 &lt;- cornyields %&gt;% filter(year == 2018) %&gt;% mutate(county_ansi = as.factor(county_ansi)) # Group by county, calculate mean for temp, and filter temp_2018 &lt;- tmaxdf %&gt;% filter(year == 2018 &amp; !is.na(tmax)) %&gt;% group_by(countyfp) %&gt;% summarize(meantmax = mean(tmax)) # Convert county to factor for the inner join temp_2018 &lt;- temp_2018 %&gt;% mutate(county_ansi = as.factor(countyfp)) # Join datasets, remove duplicate countyfp column YT_2018 &lt;- inner_join(yield_2018, temp_2018, by = &quot;county_ansi&quot;) %&gt;% select(!countyfp) YT_2018$meantsq &lt;- YT_2018$meantmax^2 YT_quad &lt;- lm(yield ~ meantmax + meantsq, YT_2018) summary(YT_2018) ## county_ansi county_name yield year meantmax meantsq ## 1 : 1 Length:93 Min. :149.5 Min. :2018 Min. :11.41 Min. :130.3 ## 3 : 1 Class :character 1st Qu.:180.2 1st Qu.:2018 1st Qu.:12.88 1st Qu.:165.8 ## 5 : 1 Mode :character Median :193.3 Median :2018 Median :13.87 Median :192.3 ## 9 : 1 Mean :193.3 Mean :2018 Mean :13.90 Mean :194.9 ## 11 : 1 3rd Qu.:211.3 3rd Qu.:2018 3rd Qu.:15.06 3rd Qu.:226.9 ## 13 : 1 Max. :226.0 Max. :2018 Max. :16.25 Max. :264.1 ## (Other):87 YT_2018$tmaxfit &lt;- YT_quad$fitted.values ggplot(YT_2018, aes(meantmax, yield)) + geom_point(color = &quot;darkgoldenrod1&quot;) + geom_line(aes(meantmax, tmaxfit), color = &quot;green4&quot;) + labs(title = &quot;Comparison of Corn Yield to Mean Annual Max Temperature, \\nAll Iowa Counties [Quadratic Fit]&quot;, x = &quot;Mean Annual Max Temperature (C)&quot;, y = &quot;Yield&quot;) From our plot, we can see that average temperatures (~13.75C) produce the highest yield across all counties in Iowa. As average max temperatures reach extremes, yields are reduced. 6.2.5 Question 4  Panel: One way to leverage multiple time series is to group all data into what is called a panel regression. Convert the county ID code (countyfp or county_ansi) into factor using as.factor, then include this variable in a regression using all counties yield and summer temperature data. How does the significance of your temperature coefficients (Tmax, Tmax^2) change? Make a plot comparing actual and fitted yields and interpret the results of your model. # Collect all summer data summer_tmax &lt;- tmaxdf %&gt;% filter(doy &gt;= 152 &amp; doy &lt;= 243) %&gt;% group_by(countyfp) %&gt;% summarize(meantmax = mean(tmax)) corn_Y &lt;- cornyieldsall %&gt;% group_by(county_name) %&gt;% select(county_ansi, county_name, yield, year) %&gt;% filter(!is.na(county_ansi) &amp; !is.na(yield)) summer_tmax$county_ansi &lt;- as.factor(summer_tmax$countyfp) corn_Y$county_ansi &lt;- as.factor(corn_Y$county_ansi) summer_tmax$tmaxsq &lt;- summer_tmax$meantmax^2 # Use yield_2018, join with summer_tmax^2 iowa_YT &lt;- inner_join(corn_Y, summer_tmax, by = &quot;county_ansi&quot;) # Add &quot;+&quot; to linear model for all yields, add the as.factor column = county_ansi panel_YT &lt;- lm(yield ~ county_ansi + meantmax + tmaxsq + year, iowa_YT) iowa_YT$panelfit &lt;- panel_YT$fitted.values - 100 # I struggled to find a good regression and functioning linear model based upon the model posted in code help. ggplot(iowa_YT, aes(yield, panelfit)) + geom_point(color = &quot;darkgoldenrod1&quot;) + geom_line(aes(yield, panelfit), color = &quot;green4&quot;) + geom_smooth(method = &quot;lm&quot;, formula = y ~ x) + labs(title = &quot;Comparison of Corn Yield to Mean Annual Max Temperature, \\nAll Iowa Counties [Panel Regression]&quot;, x = &quot;Mean Summer Max Temperature (C)&quot;, y = &quot;Yield&quot;) 6.2.6 Question 5  Soybeans: Download NASS data on soybean yields and explore either a time series relationship for a given county, the cross-sectional relationship for a given year, or a panel across all counties and years. # Finding the soybean parameter # nassqs_param_values(&quot;commodity_desc&quot;) # parameters to query on [Jefferson county, 2000 to present] param_soy &lt;- list(commodity_desc = &quot;SOYBEANS&quot;, prodn_practice_desc = &quot;ALL PRODUCTION PRACTICES&quot;, year__GE = 2000, state_alpha = &quot;IA&quot;, county_ansi = &quot;101&quot;) # Download dataset soyyieldsjeff &lt;- nassqs_yields(param_soy) ## | | | 0% | |============================= | 29% | |======================================================================= | 71% | |====================================================================================================| 100% soyyieldsjeff$Value &lt;- as.numeric(soyyieldsjeff$Value) soyyieldsjeff$yield &lt;- soyyieldsjeff$Value # Filter the dataset cornyields &lt;- select(cornyieldsall, county_ansi, county_name, yield, year) %&gt;% filter(!is.na(county_ansi) &amp; !is.na(yield)) cornyields &lt;- tibble(cornyields) ggplot(soyyieldsjeff, aes(year, yield)) + geom_point(color = &quot;darkgoldenrod1&quot;) + geom_smooth(method = lm, color = &quot;green4&quot;, fill = NA) + labs(title = &quot;Average Annual Soybean Yield in Jefferson, Iowa [Linear Fit]&quot;, x = &quot;Year&quot;, y = &quot;Yield&quot;) We can see a trend of increasing yield with each year. This makes since as a trend since populations and demand steadily increases over time. Additionally, the yield may be greater from improved farming technology. 6.2.7 Bonus: Find a package to make a county map of Iowa displaying some sort of information about yields or weather. Interpret your map. 6.2.8 Bonus #2: Challenge question - map trends in corn yields by county across Iowa. Interpret your map. Run linear model for each, grab increase (slope). Combine county value with slope, display map by counties // increase. red for (-), yellow for (0), green for (+). "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
